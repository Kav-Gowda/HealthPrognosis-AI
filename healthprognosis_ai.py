# -*- coding: utf-8 -*-
"""HealthPrognosis_AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVoKrakB7_kowe0Mot7dbo6TckgsrLLY
"""

# --- Install dependencies (Colab-compatible) ---
!pip install -q scikit-learn keras tensorflow

# --- Import libraries ---
import pandas as pd
import numpy as np

# --- Load dataset ---
# The dataset used here is the global COVID-19 dataset from Our World in Data.
covid_df = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')
print("✅ Dataset loaded successfully!")
print(f"Shape: {covid_df.shape[0]} rows × {covid_df.shape[1]} columns")

# --- Basic info ---
covid_df.info()

# --- Inspect key categorical fields ---
fields = ['iso_code', 'continent', 'location', 'tests_units']
covid_df[fields].head()

# --- Check the 'date' column ---
covid_df['date'].head()

# --- Data Type Conversion ---
fields = ['iso_code', 'continent', 'location', 'tests_units']
covid_df[fields] = covid_df[fields].astype('category')
covid_df['date'] = pd.to_datetime(covid_df['date'])

# Summary of categorical fields
covid_df[fields].describe()

# --- Filter by Country ---
# You can change "Ukraine" to any country name, e.g. "Germany" or "India"
covid_df.index = covid_df['date']
country_covid = covid_df[covid_df['location'] == "Ukraine"]
country_covid.head()

# --- Visualize Key Features ---
import matplotlib.pyplot as plt

fields = ['new_cases', 'new_cases_smoothed']
country_covid[fields].plot(title='COVID-19 New Cases Dynamics', figsize=(10,5))
plt.xlabel('Date')
plt.ylabel('Cases')
plt.show()

# --- Eliminate Missing Data ---
country_covid = country_covid[fields].dropna()
country_covid.head()

# --- Lag Correlation Function ---
def lag_correlation_ts(y, x, lag):
    """
    Calculates lag correlation for two time series.
    :param y: fixed series
    :param x: shifted series
    :param lag: lag in days
    :return: DataFrame of lag correlation coefficients
    """
    r = [0] * (lag + 1)
    y, x = y.copy(), x.copy()
    y.name, x.name = "y", "x"

    for i in range(lag + 1):
        ds = y.to_frame()
        ds = ds.join(x.shift(i), how='outer')
        r[i] = ds.corr().values[0][1]
    r = pd.DataFrame(r, columns=['Correlation'])
    r.index.name = 'Lag'
    return r

# --- Create Target Dataset and Test 30-Day Lag ---
y_dataset = country_covid['new_cases']
pd.options.display.float_format = '{:,.4f}'.format

lag_df = lag_correlation_ts(y_dataset, y_dataset, 30)
print(lag_df)

lag_df.plot(title='Lag Correlation (up to 30 days)', figsize=(8,5))
plt.grid(True)
plt.show()

# --- Time-series -> supervised transformation ---
def series_to_supervised(in_data, tar_data, n_in=1, dropnan=True, target_dep=False):
    """
    Transform a time series into a supervised learning dataset with lagged inputs.
      in_data   : DataFrame of input features
      tar_data  : Series of target
      n_in      : number of lag steps
      dropnan   : drop rows with NaNs after shifting
      target_dep: if True, include lagged target columns (t-1..t-n) as inputs
    Returns: DataFrame with lagged inputs and target at the end
    """
    n_vars = in_data.shape[1]
    cols, names = [], []

    i_start = 1 if target_dep else 0
    for i in range(i_start, n_in + 1):
        cols.append(in_data.shift(i))
        names += [f"{in_data.columns[j]}(t-{i})" for j in range(n_vars)]

    if target_dep:
        for i in range(n_in, -1, -1):
            cols.append(tar_data.shift(i))
            names += [f"{tar_data.name}(t-{i})"]
    else:
        cols.append(tar_data)
        names.append(tar_data.name)

    agg = pd.concat(cols, axis=1)
    agg.columns = names
    if dropnan:
        agg.dropna(inplace=True)
    return agg

# Build supervised dataset (input = lagged new_cases, target = current new_cases)
dataset = series_to_supervised(pd.DataFrame(y_dataset), y_dataset, n_in=14, dropnan=True, target_dep=False)
dataset.head()

# --- Split inputs/target ---
cols = dataset.columns
X, Y = dataset[cols[1:-1]], dataset[cols[-1]]
print("Input columns:", list(X.columns))
print("Target column:", Y.name)
Y.name

# --- Normalize data ---
from sklearn.preprocessing import MinMaxScaler
scaler_x = MinMaxScaler(feature_range=(0, 1))
scaler_y = MinMaxScaler(feature_range=(0, 1))

scaled_x = scaler_x.fit_transform(X)
scaled_y = scaler_y.fit_transform(Y.values.reshape(-1, 1))

# --- Train/test split (time-ordered) ---
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(scaled_x, scaled_y, test_size=0.3, shuffle=False)

# Keep real-scale references for later inverse-transform comparisons
res_train = scaler_y.inverse_transform(y_train).flatten()
res_test  = scaler_y.inverse_transform(y_test).flatten()

# --- Linear Regression ---
from sklearn.linear_model import LinearRegression
from sklearn import metrics

regressor = LinearRegression()
regressor.fit(X_train, y_train)  # y_* are scaled

# Predict and inverse-transform to real scale
y_pred_test_ln = regressor.predict(X_test)
y_pred_test_ln = scaler_y.inverse_transform(y_pred_test_ln).flatten()

print("R^2 train (scaled):", regressor.score(X_train, y_train))
print("R^2 test  (scaled):", regressor.score(X_test, y_test))
print('MAE (real):', metrics.mean_absolute_error(res_test, y_pred_test_ln))
print('MSE (real):', metrics.mean_squared_error(res_test, y_pred_test_ln))
print('RMSE (real):', np.sqrt(metrics.mean_squared_error(res_test, y_pred_test_ln)))

# --- Backpropagation Neural Network (MLP) ---
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

def BP_model(X):
    """
    Multilayer neural network with backpropagation.
    :param X: Input array (used for input_dim)
    :return: compiled Keras model
    """
    model = Sequential()
    model.add(Dense(100, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(100, kernel_initializer='normal', activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(1, kernel_initializer='normal'))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

epochs = 1000
batch_size = max(8, int(y_train.shape[0] * 0.1))  # keep stable for small sets

fitting = True
fitting_save = True

import pickle
from tensorflow.keras.models import load_model

if fitting:
    bp_model = BP_model(X_train)
    hist = bp_model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1
    )
    if fitting_save:
        bp_model.save('BP_saved_model.h5')
        with open('history.pickle', 'wb') as f:
            pickle.dump(hist.history, f)
else:
    bp_model = load_model('BP_saved_model.h5')
    with open('history.pickle', 'rb') as f:
        hist = pickle.load(f)

# If we reloaded, ensure `hist` is a dict-like history
history_dict = hist if isinstance(hist, dict) else hist.history
print("✅ BP model ready (trained or loaded).")

# --- Plot train/validation loss ---
import matplotlib.pyplot as plt

plt.figure()
plt.plot(history_dict['loss'], label='train')
plt.plot(history_dict['val_loss'], label='val')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# --- ANN predictions (inverse-transformed) ---
res_tr = bp_model.predict(X_train, verbose=0)
res_ts = bp_model.predict(X_test,  verbose=0)

res_train_ANN = scaler_y.inverse_transform(res_tr.reshape(-1, 1)).flatten()
res_test_ANN  = scaler_y.inverse_transform(res_ts.reshape(-1, 1)).flatten()

# Compare Linear Regression vs ANN (use real-scale ground truth)
print("Correlation (train):", np.corrcoef(res_train, res_train_ANN)[0, 1])
print("Correlation (test): ", np.corrcoef(res_test,  res_test_ANN)[0, 1])
print('MAE (real):', metrics.mean_absolute_error(res_test, res_test_ANN))
print('MSE (real):', metrics.mean_squared_error(res_test, res_test_ANN))
print('RMSE (real):', np.sqrt(metrics.mean_squared_error(res_test, res_test_ANN)))

# --- LSTM: reshape inputs to 3D (samples, timesteps, features) ---
# Using 1 timestep and 14 features (matching your X with 14 lag columns)
train_x_LSTM = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
test_x_LSTM  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
train_x_LSTM.shape, test_x_LSTM.shape

# --- Define LSTM model (7 LSTM units + small dense head) ---
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import pickle

batch_size = max(8, int(y_train.shape[0] * 0.1))
epochs = 400
fitting = True
fitting_save = True

lstm_model = Sequential()
lstm_model.add(LSTM(7, input_shape=(train_x_LSTM.shape[1], train_x_LSTM.shape[2])))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(7, kernel_initializer='normal', activation='relu'))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(1))
lstm_model.compile(loss='mean_squared_error', optimizer='adam')

if fitting:
    history_lstm = lstm_model.fit(
        train_x_LSTM, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(test_x_LSTM, y_test),
        verbose=1,
        shuffle=False
    )
    if fitting_save:
        lstm_model.save('LSTM_saved_model.h5')
        with open('history_LSTM.pickle', 'wb') as f:
            pickle.dump(history_lstm.history, f)
else:
    from tensorflow.keras.models import load_model
    lstm_model = load_model('LSTM_saved_model.h5')
    with open('history_LSTM.pickle', 'rb') as f:
        history_lstm = pickle.load(f)

# Ensure we have a plain dict for plotting
hist_lstm_dict = history_lstm if isinstance(history_lstm, dict) else history_lstm.history
print("✅ LSTM model ready (trained or loaded).")

# --- Plot loss curves (train vs val) ---
import matplotlib.pyplot as plt

plt.figure()
plt.plot(hist_lstm_dict['loss'], label='train')
plt.plot(hist_lstm_dict['val_loss'], label='val')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()
plt.show()

# --- Forecasts (inverse-transformed to real scale) ---
res_tr_LSTM = lstm_model.predict(train_x_LSTM, verbose=0)
res_ts_LSTM = lstm_model.predict(test_x_LSTM,  verbose=0)

res_train_LSTM = scaler_y.inverse_transform(res_tr_LSTM).flatten()
res_test_LSTM  = scaler_y.inverse_transform(res_ts_LSTM).flatten()

# Accuracy (real-scale)
from sklearn import metrics
print("Correlation train:", np.corrcoef(res_train, res_train_LSTM)[0, 1])
print("Correlation test: ", np.corrcoef(res_test,  res_test_LSTM)[0, 1])
print('MAE (real):',  metrics.mean_absolute_error(res_test,  res_test_LSTM))
print('MSE (real):',  metrics.mean_squared_error(res_test,   res_test_LSTM))
print('RMSE (real):', np.sqrt(metrics.mean_squared_error(res_test, res_test_LSTM)))

# --- Compare Linear vs ANN vs LSTM on test set ---
res_pred_test_ln   = pd.Series(y_pred_test_ln,   name='Predicted test Linear Model')
res_pred_test_ANN  = pd.Series(res_test_ANN,     name='Predicted test ANN')
res_pred_test_LSTM = pd.Series(res_test_LSTM,    name='Predicted test LSTM')

df_2 = pd.DataFrame({
    'Actual test': res_test,
    'Linear Model': res_pred_test_ln,
    'ANN Model': res_pred_test_ANN,
    'LSTM Model': res_pred_test_LSTM,
})
# Align index to the trailing part of the original dataset (like your earlier code)
df_2.index = dataset.index[len(dataset) - len(res_test):]

ax = df_2.plot(title='Test Set: Actual vs Predictions (Linear, ANN, LSTM)', figsize=(12,6))
ax.set_xlabel('Date')
ax.set_ylabel('Cases')
plt.show()